# AegisIDE Enforcement Engine

## üéØ Purpose

**Make AI rule violations IMPOSSIBLE, not just penalized.**

This enforcement engine creates **hard constraints** that physically block AI from breaking rules, rather than relying on the AI to voluntarily follow rules.

## Enforcement Layer

Programmatic enforcement of constitutional rules, workflow validation, and autonomous execution policies.

## Purpose

This directory contains enforcement mechanisms that provide **hard governance** (code-based blocks) instead of **soft governance** (AI guidance).

## Status

‚úÖ **Phase 1 Complete** - Python enforcement layer implemented (v4.0.1)
üü° **Phase 2 In Progress** - CLI integration and automatic enforcement

## Implemented Components

1. ‚úÖ **ConfidenceCalculator** - Automatic confidence scoring with 4-factor weighted formula
2. ‚úÖ **MCPChainValidator** - Enforces 4-step MCP chain with automatic penalties
3. ‚úÖ **AtomicTransactionManager** - ACID properties for schema updates with rollback
4. ‚úÖ **ConstitutionalFlowEnforcer** - Validates Legislature‚ÜíExecutive‚ÜíJudiciary‚ÜíBodies sequence
5. ‚úÖ **AegisEnforcer** - Main coordinator class integrating all enforcement modules

## Planned Components (Phase 2)

6. üü° **AccessController** - Permission-based action blocking
7. üü° **JudicialReviewer** - Constitutional compliance validation
8. üü° **AuditTrail** - Immutable action history with cryptographic signatures

## Files

- `aegis_enforcer.py` - **NEW** Python enforcement implementation (395 lines)
- `enforcement-engine.json` - Enforcement configuration (metadata)
- `README.md` - This file

## Usage

### Python Module

```python
from enforcement.aegis_enforcer import AegisEnforcer

# Initialize enforcer
enforcer = AegisEnforcer("runtime/memory")

# Enforce confidence-based execution
factors = {
    "keyword_match": 0.9,
    "semantic_similarity": 0.8,
    "memory_hit": 0.7,
    "historical_accuracy": 0.9
}
can_execute, reason = enforcer.enforce_confidence_based_execution(factors, "update_readme")
if not can_execute:
    raise ConfidenceTooLowError(reason)

# Enforce MCP chain
executed_steps = {
    "sequential_thinking": True,
    "memory_query": True,
    "research": True,
    "pattern_storage": False  # Missing!
}
is_valid, penalty = enforcer.enforce_mcp_chain(executed_steps, 0.85)
# Penalty automatically applied to progress.json

# Enforce atomic updates
updates = {
    "activeContext": {"last_action": "test"},
    "progress": {"tasks_completed": 5}
}
success = enforcer.enforce_atomic_update(updates)
# Auto-rollback on any failure
```

### CLI Wrapper (Coming Soon)

```bash
# Validate confidence before action
aegis enforce confidence --action "update_readme" --factors '{"keyword_match": 0.9}'

# Validate MCP chain
aegis validate mcp-chain --steps '{"sequential_thinking": true}'

# Atomic update
aegis update atomic --schemas '{"activeContext": {...}}'
```

## Architecture Changes

**Before (v4.0.0):**
- Documentation-driven (AI interprets guidance)
- Soft governance (RL penalties, honor system)
- Self-verification (no blocks)

**After (v4.0.1+):**
- Code-driven enforcement (programmatic blocks)
- Hard governance (exceptions raised on violations)
- Automatic validation (before actions execute)

## Integration Points

1. **AI Wrapper** - Intercepts AI actions, enforces rules BEFORE execution
2. **CLI Tools** - Command-line enforcement for scripts/automation
3. **MCP Integration** - Hooks into @mcp:json-jq for automatic validation
4. **Workflow Engine** - Executes workflow steps with enforcement checks

## Development Roadmap

- [x] Phase 1: Core enforcement modules (v4.0.1)
- [ ] Phase 2: CLI wrapper and automation
- [ ] Phase 3: AI wrapper integration
- [ ] Phase 4: Real-time enforcement during IDE usage
- [ ] Phase 5: Cryptographic audit trail
- [ ] Phase 6: Multi-user governance with voting

## Testing

```bash
# Run enforcement layer tests
python -m pytest enforcement/tests/

# Run example
python enforcement/aegis_enforcer.py
```

## Contributing

When adding new enforcement mechanisms:
1. Add enforcement class to `aegis_enforcer.py`
2. Integrate with `AegisEnforcer` main class
3. Add tests to `tests/` directory
4. Update this README
5. Raise appropriate exceptions for violations
6. Log all enforcement actions to progress.json

---

## üîí Enforcement Mechanisms

### 1. **Pre-Execution Gates** (AI Cannot Start Without Passing)
- ‚úÖ Bootstrap loaded check
- ‚úÖ Confidence calculated check
- ‚úÖ Memory queried check
- ‚úÖ Intent detected check
- ‚úÖ MCP chain started check

**Result:** AI physically blocked from executing tasks without passing all gates.

### 2. **Runtime Hooks** (AI Actions Intercepted)
- üö´ File system hooks ‚Üí Block .aegiside/ access
- üö´ Tool selection hooks ‚Üí Block wrong tools on JSON
- üö´ Intent validation hooks ‚Üí Block mismatched file operations
- üö´ Confidence threshold hooks ‚Üí Block low-confidence execution
- üö´ Atomic sync hooks ‚Üí Auto-rollback partial updates
- üö´ MCP chain hooks ‚Üí Block incomplete chains

**Result:** AI actions physically blocked if rules violated.

### 3. **Post-Execution Validators** (AI Output Rejected If Invalid)
- ‚ùå Schema validation ‚Üí Reject invalid JSON
- ‚ùå Violation logging ‚Üí Reject unlogged violations
- ‚ùå Audit trail ‚Üí Reject unaudited actions

**Result:** Invalid AI outputs physically rejected.

---

## üõ°Ô∏è Bypass Prevention

### AI CANNOT:
1. ‚ùå Modify enforcement rules (file is READ-ONLY)
2. ‚ùå Disable hooks (run at system level)
3. ‚ùå Skip gates (mandatory system checks)
4. ‚ùå Fake compliance (checks verify actual state)

---

## üìä Integration with Architecture

```
User Query
    ‚Üì
[GATE 1: Bootstrap Check] ‚Üí HALT if not loaded
    ‚Üì
[GATE 2: Confidence Check] ‚Üí HALT if not calculated
    ‚Üì
[GATE 3: Memory Check] ‚Üí HALT if not queried
    ‚Üì
[GATE 4: Intent Check] ‚Üí HALT if not detected
    ‚Üì
[GATE 5: MCP Chain Check] ‚Üí HALT if not started
    ‚Üì
AI Execution (with runtime hooks monitoring)
    ‚Üì
[VALIDATOR 1: Schema Check] ‚Üí REJECT if invalid
    ‚Üì
[VALIDATOR 2: Logging Check] ‚Üí REJECT if not logged
    ‚Üì
[VALIDATOR 3: Audit Check] ‚Üí REJECT if not audited
    ‚Üì
Response Returned
```

---

## üîß How It Works

### Example: AI tries to access runtime/memory/history/progress.json

1. **Tier-Aware Tool Enforcement:**
   - Detects file path is `runtime/memory/`
   - **BLOCKS if using wrong tool (jq instead of @mcp:json-jq)**
   - Returns error: "Use @mcp:json-jq for runtime/memory/ ONLY."
   - Applies -50 penalty to progress.json

2. **AI Must Use Correct Tool:**
   - Must use @mcp:json-jq for runtime/memory/
   - jq allowed for core/ and knowledge/
   - Penalty automatically logged to history/progress.json

### Example: AI tries to use Read() on JSON file

1. **Tool Selection Hook Intercepts:**
   - Detects Read() called on .json file
   - **BLOCKS operation immediately**
   - Returns error: "Use @mcp:json-jq for JSON files only."
   - Applies -30 penalty to progress.json

2. **AI Cannot Proceed:**
   - Read() operation physically blocked
   - AI must use @mcp:json-jq instead
   - Penalty automatically logged

### Example: AI tries to create file with UPDATE intent

1. **Intent Validation Hook Intercepts:**
   - Detects file creation operation
   - Checks activeContext.file_operation_intent.last_intent_type == "UPDATE"
   - **BLOCKS operation immediately**
   - Returns error: "Intent is UPDATE, cannot create files. Only modify existing."
   - Applies -100 penalty to progress.json

2. **AI Cannot Proceed:**
   - File creation physically blocked
   - AI must modify existing file instead
   - Penalty automatically logged

---

## üìã Enforcement Checklist

### Pre-Execution (5 Gates)
- [ ] Bootstrap loaded
- [ ] Confidence calculated
- [ ] Memory queried
- [ ] Intent detected
- [ ] MCP chain started

### Runtime (6 Hooks)
- [ ] File access monitored
- [ ] Tool usage monitored
- [ ] Intent validated
- [ ] Confidence threshold enforced
- [ ] Atomic sync enforced
- [ ] MCP chain completion enforced

### Post-Execution (3 Validators)
- [ ] Schema validation passed
- [ ] Violation logging verified
- [ ] Audit trail verified

---

## üéØ Result

**AI has NO CHOICE but to follow rules.**

Rules are enforced at system level, not AI level. AI cannot bypass, disable, or fake compliance.

**This is AI-PROOF enforcement.**
