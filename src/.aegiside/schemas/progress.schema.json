{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["schema_version", "reinforcement_learning"],
  "properties": {
    "schema_version": {"type": "string", "pattern": "^\\d+\\.\\d+\\.\\d+$"},
    "last_updated": {"type": "string", "format": "date-time"},
    "metrics": {
      "type": "object",
      "description": "SOURCE OF TRUTH - activeContext.metrics syncs total_rl_score from here",
      "required": ["total_rl_score", "tasks_completed", "commits"],
      "properties": {
        "total_rl_score": {"type": "number"},
        "tasks_completed": {"type": "integer", "minimum": 0},
        "tasks_failed": {"type": "integer", "minimum": 0},
        "commits": {"type": "integer", "minimum": 0}
      }
    },
    "reinforcement_learning": {
      "type": "array",
      "description": "TOP-APPEND MANDATORY: Newest transaction MUST be at [0]. RL ledger with PPO+GAE architecture.",
      "items": {
        "type": "object",
        "properties": {
          "tx_id": {"type": "string"},
          "timestamp": {"type": "string", "format": "date-time"},
          "category": {"type": "string"},
          "reward": {"type": "number"},
          "penalty": {"type": "number"},
          "source_file": {"type": "string"},
          "description": {"type": "string"},
          "gae_advantage": {"type": "number", "description": "GAE advantage estimation (gamma=1.0, lambda=1.0)"},
          "kl_divergence": {"type": "number", "description": "KL penalty (kl_coef=0.005)"}
        }
      }
    },
    "rl_architecture": {
      "type": "object",
      "description": "PPO+GAE RL system configuration",
      "properties": {
        "algorithm": {"type": "string", "default": "PPO"},
        "kl_coefficient": {"type": "number", "default": 0.005},
        "gae_params": {
          "type": "object",
          "properties": {
            "gamma": {"type": "number", "default": 1.0},
            "lambda": {"type": "number", "default": 1.0}
          }
        },
        "exploit_explore_ratio": {
          "type": "object",
          "properties": {
            "exploitation": {"type": "number", "default": 0.7},
            "exploration": {"type": "number", "default": 0.3}
          }
        },
        "value_network": {"type": "string", "default": "multi-branch"},
        "reward_design": {"type": "string", "default": "automated_llm"},
        "n_step_td_learning": {
          "type": "object",
          "description": "Temporal Difference n-step returns for credit assignment across multi-step tasks. Formula: V(s) ← V(s) + α[r + γV(s') - V(s)]",
          "properties": {
            "n_steps": {"type": "integer", "default": 3, "description": "Lookahead horizon for TD(n)"},
            "discount_gamma": {"type": "number", "default": 0.99, "description": "Discount factor for future rewards"},
            "td_error_threshold": {"type": "number", "default": 0.1, "description": "Trigger learning update when |TD_error| > threshold"},
            "sub_goal_rewards": {
              "type": "array",
              "description": "Track intermediate sub-goal rewards for n-step returns",
              "items": {
                "type": "object",
                "properties": {
                  "step": {"type": "integer"},
                  "gae_advantage": {"type": "number", "description": "GAE advantage: Σ(γλ)^k × δ_{t+k}"},
                  "kl_divergence": {"type": "number", "description": "KL(π_new || π_ref) for policy stability"},
                  "rl_computation": {
                    "type": "object",
                    "description": "Actual RL formula calculations for this transaction",
                    "properties": {
                      "td_error": {"type": "number", "description": "r_t + γV(s_{t+1}) - V(s_t)"},
                      "value_updated": {"type": "number", "description": "V(s) + α*TD_error"},
                      "policy_probabilities": {"type": "object", "description": "Softmax π(a) = exp(Q/τ)/Σexp(Q/τ)"},
                      "monte_carlo_return": {"type": "number", "description": "G_t = Σγ^k*r_{t+k}"}
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "current_focus": {
      "type": "object",
      "properties": {
        "task": {"type": "string"},
        "tests_ready": {"type": "array", "items": {"type": "string"}}
      }
    },
    "resource_usage": {
      "type": "object",
      "description": "UX Gap 6 fix: Track API calls and costs to prevent budget surprises",
      "properties": {
        "api_calls": {
          "type": "object",
          "properties": {
            "context7": {"type": "integer", "minimum": 0},
            "exa": {"type": "integer", "minimum": 0},
            "fetch": {"type": "integer", "minimum": 0}
          }
        },
        "estimated_cost_usd": {"type": "number", "minimum": 0, "description": "Running total cost estimate"},
        "budget_limit_usd": {"type": "number", "default": 10.0, "description": "User-defined budget limit"},
        "alert_threshold": {"type": "number", "default": 0.8, "description": "Alert when cost reaches 80% of budget"},
        "budget_exceeded": {"type": "boolean", "default": false},
        "last_alert": {"type": "string", "format": "date-time"}
      }
    },
    "autonomy_settings": {
      "type": "object",
      "description": "UX Gap 10 fix: User-adjustable autonomy level for different work contexts",
      "properties": {
        "level": {"type": "integer", "minimum": 0, "maximum": 100, "default": 99, "description": "0=ask everything, 99=autonomous"},
        "temporary_override": {"type": "integer", "minimum": 0, "maximum": 100, "description": "Temporary autonomy level (e.g., 50 for production deploy)"},
        "override_reason": {"type": "string", "description": "Why autonomy was reduced"},
        "expires_at": {"type": "string", "format": "date-time", "description": "When to restore original autonomy"},
        "last_changed": {"type": "string", "format": "date-time"}
      }
    },
    "notification_preferences": {
      "type": "object",
      "description": "UX Gap 16 fix: User control over notification verbosity",
      "properties": {
        "verbosity": {"type": "string", "enum": ["silent", "minimal", "normal", "verbose"], "default": "normal"},
        "notify_on": {
          "type": "object",
          "properties": {
            "task_start": {"type": "boolean", "default": true},
            "task_complete": {"type": "boolean", "default": true},
            "error": {"type": "boolean", "default": true},
            "high_risk_action": {"type": "boolean", "default": true},
            "budget_warning": {"type": "boolean", "default": true},
            "validation_fail": {"type": "boolean", "default": true},
            "progress_updates": {"type": "boolean", "default": true},
            "rl_score_changes": {"type": "boolean", "default": false}
          }
        }
      }
    },
    "rl_model_state": {
      "type": "object",
      "description": "Gap 17 fix: RL model persistence and rollback capability",
      "properties": {
        "model_version": {"type": "string", "description": "e.g., v1.2.3"},
        "last_checkpoint": {"type": "string", "description": "Git commit SHA of last model snapshot"},
        "checkpoint_timestamp": {"type": "string", "format": "date-time"},
        "policy_snapshot_ref": {"type": "string", "description": "Path to serialized policy state"},
        "value_network_snapshot_ref": {"type": "string", "description": "Path to value network weights"},
        "performance_at_checkpoint": {"type": "number", "description": "Total RL score at snapshot"}
      }
    },
    "rl_runtime_controls": {
      "type": "object",
      "description": "Gap 18 fix: User-adjustable exploration/exploitation balance",
      "properties": {
        "exploit_ratio": {"type": "number", "minimum": 0, "maximum": 1, "default": 0.70, "description": "Reuse proven patterns"},
        "explore_ratio": {"type": "number", "minimum": 0, "maximum": 1, "default": 0.30, "description": "Try new approaches"},
        "epsilon_greedy": {"type": "number", "minimum": 0, "maximum": 1, "default": 0.1, "description": "Random exploration probability"},
        "epsilon_decay": {"type": "number", "default": 0.995, "description": "Decay rate per task"},
        "user_adjustable": {"type": "boolean", "default": true},
        "last_adjusted": {"type": "string", "format": "date-time"}
      }
    },
    "reward_breakdown": {
      "type": "object",
      "description": "Gap 19 fix: Transparent reward function calculation",
      "properties": {
        "last_task_rewards": {
          "type": "object",
          "properties": {
            "base_score": {"type": "number", "description": "Task completion base"},
            "quality_bonus": {"type": "number", "description": "Validation pass, EMD compliance"},
            "speed_bonus": {"type": "number", "description": "Efficiency reward"},
            "user_rating_bonus": {"type": "number", "description": "Explicit user feedback"},
            "pattern_reuse_bonus": {"type": "number", "description": "+20 per reuse"},
            "mcp_integration_bonus": {"type": "number", "description": "+10 per complete chain"},
            "total": {"type": "number", "description": "Sum of all components"}
          }
        },
        "calculation_formula": {"type": "string", "default": "total = base + quality + speed + user_rating + pattern_reuse + mcp"}
      }
    },
    "value_network_branches": {
      "type": "object",
      "description": "Gap 20 fix: Multi-branch value network weighting details",
      "properties": {
        "branch_weights": {
          "type": "object",
          "properties": {
            "task_success": {"type": "number", "default": 0.30, "description": "Did task complete?"},
            "code_quality": {"type": "number", "default": 0.25, "description": "Validation, EMD, security"},
            "user_satisfaction": {"type": "number", "default": 0.20, "description": "User rating, feedback"},
            "efficiency": {"type": "number", "default": 0.15, "description": "Time, resource usage"},
            "learning": {"type": "number", "default": 0.10, "description": "Pattern creation, knowledge graph"}
          }
        },
        "weight_sum_constraint": {"type": "number", "const": 1.0, "description": "Weights must sum to 1.0"},
        "auto_adjust": {"type": "boolean", "default": false, "description": "LLM adjusts based on success patterns"},
        "last_adjustment": {"type": "string", "format": "date-time"}
      }
    }
  }
}
